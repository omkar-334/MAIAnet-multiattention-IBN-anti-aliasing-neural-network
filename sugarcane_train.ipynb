{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ea79405-3e58-4040-9d4b-79697654f227",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch pandas torchvision scikit-learn tqdm kaggle -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78d482ea-0e22-4b07-a205-1d259fb90a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p ~/.kaggle\n",
    "!mv kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b990dab2-2199-4cc7-b547-bd3ad9a34014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading cassava-leaf-disease-classification.zip to /home/ubuntu\n",
      "100%|██████████████████████████████████████▉| 5.76G/5.76G [00:40<00:00, 230MB/s]\n",
      "100%|███████████████████████████████████████| 5.76G/5.76G [00:40<00:00, 153MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle datasets download nirmalsankalana/sugarcane-leaf-disease-dataset\n",
    "!unzip -q sugarcane-leaf-disease-dataset.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6620ca06-f014-4cdb-a636-0f1de07297b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# Define paths\n",
    "data_root = \"data\"\n",
    "images_dir = os.path.join(data_root, \"images\")\n",
    "\n",
    "# Create images directory if it doesn't exist\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "# List to store image paths and labels\n",
    "dataset = []\n",
    "\n",
    "# Loop through each subfolder\n",
    "for subfolder in os.listdir(data_root):\n",
    "    subfolder_path = os.path.join(data_root, subfolder)\n",
    "    \n",
    "    # Ensure it's a directory\n",
    "    if os.path.isdir(subfolder_path) and subfolder != \"images\":\n",
    "        # Loop through images inside the subfolder\n",
    "        for image in os.listdir(subfolder_path):\n",
    "            old_image_path = os.path.join(subfolder_path, image)\n",
    "            \n",
    "            # Ensure it's a file (image)\n",
    "            if os.path.isfile(old_image_path):\n",
    "                # Define new image path in \"data/images\" directory\n",
    "                new_image_path = os.path.join(images_dir, image)\n",
    "                \n",
    "                # If filename already exists, rename it to avoid conflicts\n",
    "                if os.path.exists(new_image_path):\n",
    "                    base, ext = os.path.splitext(image)\n",
    "                    counter = 1\n",
    "                    while os.path.exists(new_image_path):\n",
    "                        new_image_path = os.path.join(images_dir, f\"{base}_{counter}{ext}\")\n",
    "                        counter += 1\n",
    "                \n",
    "                # Move image\n",
    "                shutil.move(old_image_path, new_image_path)\n",
    "\n",
    "                # Append to dataset with updated path and original label\n",
    "                dataset.append({\"image_path\": new_image_path, \"label\": subfolder})\n",
    "\n",
    "        # Optionally remove empty subfolder after moving images\n",
    "        os.rmdir(subfolder_path)\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "df = df.rename(columns={'image_path':'image_id'})\n",
    "df[\"image_id\"] = df[\"image_id\"].str.replace(\"data/images/\", \"\", regex=False)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"label\"] = label_encoder.fit_transform(df[\"label\"])\n",
    "\n",
    "df.to_csv(os.path.join(data_root, \"dataset.csv\"), index=False)\n",
    "\n",
    "label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1282e13-654a-4974-b4b9-3cf848b99bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((448, 448)),  # Resize to input size of MaiaNet\n",
    "        transforms.RandomHorizontalFlip(p=0.5),  # Horizontal flipping\n",
    "        transforms.RandomVerticalFlip(p=0.5),  # Vertical flipping\n",
    "        transforms.ToTensor(),  # Convert to tensor before adding noise\n",
    "        transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.05),  # Add Gaussian noise\n",
    "        transforms.Lambda(lambda x: transforms.functional.erase(x, i=0, j=0, h=50, w=50, v=0.0)),  # Add cutout\n",
    "    ]\n",
    ")\n",
    "\n",
    "# df = pd.read_csv(\"train.csv\")\n",
    "# df = pd.read_csv(\"cassava_data/train.csv\")\n",
    "\n",
    "# print(df.label.value_counts())\n",
    "# balanced_df = pd.DataFrame()\n",
    "\n",
    "# for label in df[\"label\"].unique():\n",
    "#     label_df = df[df[\"label\"] == label]\n",
    "#     if len(label_df) > 1000:\n",
    "#         _, sampled_df = train_test_split(label_df, test_size=500, random_state=42, stratify=label_df[\"label\"])\n",
    "#     balanced_df = pd.concat([balanced_df, sampled_df])\n",
    "\n",
    "\n",
    "class Dataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = \"data/images\"\n",
    "        self.transform = transform\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.dataframe.iloc[idx][\"image_id\"]\n",
    "        label = self.dataframe.iloc[idx][\"label\"]\n",
    "        image_path = os.path.join(self.image_dir, image_id)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "        # Move tensors to GPU if available\n",
    "        image = image.to(device)\n",
    "        label = torch.tensor(label, device=device)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2ba8c31-2cf8-4315-a109-bb206a542f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label\"])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df[\"label\"])\n",
    "\n",
    "train_dataset = Dataset(train_df)\n",
    "test_dataset = Dataset(test_df)\n",
    "val_dataset = Dataset(val_df)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f478e778-23ed-44bd-a407-09d837e63d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import ResNet\n",
    "\n",
    "\n",
    "def get_freq_indices(method):\n",
    "    assert method in [\"top1\", \"top2\", \"top4\", \"top8\", \"top16\", \"top32\", \"bot1\", \"bot2\", \"bot4\", \"bot8\", \"bot16\", \"bot32\", \"low1\", \"low2\", \"low4\", \"low8\", \"low16\", \"low32\"]\n",
    "    num_freq = int(method[3:])\n",
    "    if \"top\" in method:\n",
    "        all_top_indices_x = [0, 0, 6, 0, 0, 1, 1, 4, 5, 1, 3, 0, 0, 0, 3, 2, 4, 6, 3, 5, 5, 2, 6, 5, 5, 3, 3, 4, 2, 2, 6, 1]\n",
    "        all_top_indices_y = [0, 1, 0, 5, 2, 0, 2, 0, 0, 6, 0, 4, 6, 3, 5, 2, 6, 3, 3, 3, 5, 1, 1, 2, 4, 2, 1, 1, 3, 0, 5, 3]\n",
    "        mapper_x = all_top_indices_x[:num_freq]\n",
    "        mapper_y = all_top_indices_y[:num_freq]\n",
    "    elif \"low\" in method:\n",
    "        all_low_indices_x = [0, 0, 1, 1, 0, 2, 2, 1, 2, 0, 3, 4, 0, 1, 3, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 6, 1, 2, 3, 4]\n",
    "        all_low_indices_y = [0, 1, 0, 1, 2, 0, 1, 2, 2, 3, 0, 0, 4, 3, 1, 5, 4, 3, 2, 1, 0, 6, 5, 4, 3, 2, 1, 0, 6, 5, 4, 3]\n",
    "        mapper_x = all_low_indices_x[:num_freq]\n",
    "        mapper_y = all_low_indices_y[:num_freq]\n",
    "    elif \"bot\" in method:\n",
    "        all_bot_indices_x = [6, 1, 3, 3, 2, 4, 1, 2, 4, 4, 5, 1, 4, 6, 2, 5, 6, 1, 6, 2, 2, 4, 3, 3, 5, 5, 6, 2, 5, 5, 3, 6]\n",
    "        all_bot_indices_y = [6, 4, 4, 6, 6, 3, 1, 4, 4, 5, 6, 5, 2, 2, 5, 1, 4, 3, 5, 0, 3, 1, 1, 2, 4, 2, 1, 1, 5, 3, 3, 3]\n",
    "        mapper_x = all_bot_indices_x[:num_freq]\n",
    "        mapper_y = all_bot_indices_y[:num_freq]\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return mapper_x, mapper_y\n",
    "\n",
    "\n",
    "class MultiSpectralAttentionLayer(torch.nn.Module):\n",
    "    def __init__(self, channel, dct_h, dct_w, reduction=16, freq_sel_method=\"top16\"):\n",
    "        super(MultiSpectralAttentionLayer, self).__init__()\n",
    "        self.reduction = reduction\n",
    "        self.dct_h = dct_h\n",
    "        self.dct_w = dct_w\n",
    "\n",
    "        mapper_x, mapper_y = get_freq_indices(freq_sel_method)\n",
    "        self.num_split = len(mapper_x)\n",
    "        mapper_x = [temp_x * (dct_h // 7) for temp_x in mapper_x]\n",
    "        mapper_y = [temp_y * (dct_w // 7) for temp_y in mapper_y]\n",
    "        # make the frequencies in different sizes are identical to a 7x7 frequency space\n",
    "        # eg, (2,2) in 14x14 is identical to (1,1) in 7x7\n",
    "\n",
    "        self.dct_layer = MultiSpectralDCTLayer(dct_h, dct_w, mapper_x, mapper_y, channel)\n",
    "        self.fc = nn.Sequential(nn.Linear(channel, channel // reduction, bias=False), nn.ReLU(), nn.Linear(channel // reduction, channel, bias=False), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        n, c, h, w = x.shape\n",
    "        x_pooled = x\n",
    "        if h != self.dct_h or w != self.dct_w:\n",
    "            x_pooled = torch.nn.functional.adaptive_avg_pool2d(x, (self.dct_h, self.dct_w))\n",
    "            # If you have concerns about one-line-change, don't worry.   :)\n",
    "            # In the ImageNet models, this line will never be triggered.\n",
    "            # This is for compatibility in instance segmentation and object detection.\n",
    "        y = self.dct_layer(x_pooled)\n",
    "\n",
    "        y = self.fc(y).view(n, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "\n",
    "class MultiSpectralDCTLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Generate dct filters\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, height, width, mapper_x, mapper_y, channel):\n",
    "        super(MultiSpectralDCTLayer, self).__init__()\n",
    "\n",
    "        assert len(mapper_x) == len(mapper_y)\n",
    "        assert channel % len(mapper_x) == 0\n",
    "\n",
    "        self.num_freq = len(mapper_x)\n",
    "\n",
    "        # fixed DCT init\n",
    "        self.register_buffer(\"weight\", self.get_dct_filter(height, width, mapper_x, mapper_y, channel))\n",
    "\n",
    "        # fixed random init\n",
    "        # self.register_buffer('weight', torch.rand(channel, height, width))\n",
    "\n",
    "        # learnable DCT init\n",
    "        # self.register_parameter('weight', self.get_dct_filter(height, width, mapper_x, mapper_y, channel))\n",
    "\n",
    "        # learnable random init\n",
    "        # self.register_parameter('weight', torch.rand(channel, height, width))\n",
    "\n",
    "        # num_freq, h, w\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) == 4, \"x must been 4 dimensions, but got \" + str(len(x.shape))\n",
    "        # n, c, h, w = x.shape\n",
    "\n",
    "        x = x * self.weight\n",
    "\n",
    "        result = torch.sum(x, dim=[2, 3])\n",
    "        return result\n",
    "\n",
    "    def build_filter(self, pos, freq, POS):\n",
    "        result = math.cos(math.pi * freq * (pos + 0.5) / POS) / math.sqrt(POS)\n",
    "        if freq == 0:\n",
    "            return result\n",
    "        else:\n",
    "            return result * math.sqrt(2)\n",
    "\n",
    "    def get_dct_filter(self, tile_size_x, tile_size_y, mapper_x, mapper_y, channel):\n",
    "        dct_filter = torch.zeros(channel, tile_size_x, tile_size_y)\n",
    "\n",
    "        c_part = channel // len(mapper_x)\n",
    "\n",
    "        for i, (u_x, v_y) in enumerate(zip(mapper_x, mapper_y)):\n",
    "            for t_x in range(tile_size_x):\n",
    "                for t_y in range(tile_size_y):\n",
    "                    dct_filter[i * c_part : (i + 1) * c_part, t_x, t_y] = self.build_filter(t_x, u_x, tile_size_x) * self.build_filter(t_y, v_y, tile_size_y)\n",
    "\n",
    "        return dct_filter\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "    \n",
    "import torch.nn as nn\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torchvision.models import ResNet\n",
    "\n",
    "\n",
    "class SELayer(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SELayer, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(nn.Linear(channel, channel // reduction, bias=False), nn.ReLU(), nn.Linear(channel // reduction, channel, bias=False), nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y.expand_as(x)\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class MaiaNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MaiaNet, self).__init__()\n",
    "        self.head = HeadBlock(3, 64)  # Input: 448×448×3 -> 112×112×64\n",
    "        self.anti_aliasing_1 = AntiAliasingBlock(64, 64, downsample=False)  # 112×112×64 -> 112×112×64\n",
    "        self.maia_1 = MaiaBlock(64, 256)  # 112×112×64 -> 112×112×256\n",
    "        self.anti_aliasing_2 = AntiAliasingBlock(256, 512, downsample=True)  # 112×112×256 -> 56×56×512\n",
    "        self.maia_2 = MaiaBlock(512, 512)  # 56×56×512 -> 56×56×512\n",
    "        self.anti_aliasing_3 = AntiAliasingBlock(512, 1024, downsample=True)  # 56×56×512 -> 28×28×1024\n",
    "        self.maia_3 = MaiaBlock(1024, 1024)  # 28×28×1024 -> 28×28×1024\n",
    "        self.maia_4 = MaiaBlock(1024, 2048, downsample=True)  # 14×14×2048 -> 14×14×2048\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))  # Converts 14x14x2048 to 1x1x2048\n",
    "        self.fc = nn.Linear(2048, num_classes)  # Fully connected layer (2048 -> num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "        if verbose:\n",
    "            print(\"Input:\", x.shape)\n",
    "        x = self.head(x)\n",
    "        if verbose:\n",
    "            print(\"Head:\", x.shape)\n",
    "        x = self.anti_aliasing_1(x)\n",
    "        if verbose:\n",
    "            print(\"Anti-aliasing 1:\", x.shape)\n",
    "        x = self.maia_1(x)\n",
    "        if verbose:\n",
    "            print(\"MAIA 1:\", x.shape)\n",
    "        x = self.anti_aliasing_2(x)\n",
    "        if verbose:\n",
    "            print(\"Anti-aliasing 2:\", x.shape)\n",
    "        x = self.maia_2(x)\n",
    "        if verbose:\n",
    "            print(\"MAIA 2:\", x.shape)\n",
    "        x = self.anti_aliasing_3(x)\n",
    "        if verbose:\n",
    "            print(\"Anti-aliasing 3:\", x.shape)\n",
    "        x = self.maia_3(x)\n",
    "        if verbose:\n",
    "            print(\"MAIA 3:\", x.shape)\n",
    "        x = self.maia_4(x)\n",
    "        if verbose:\n",
    "            print(\"MAIA 4:\", x.shape)\n",
    "\n",
    "        x = self.global_pool(x)  # Shape: (batch_size, 2048, 1, 1)\n",
    "        x = torch.flatten(x, 1)  # Shape: (batch_size, 2048)\n",
    "        x = self.fc(x)  # Shape: (batch_size, num_classes)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class HeadBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(HeadBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=7, padding=3, stride=2)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiAttention(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(MultiAttention, self).__init__()\n",
    "\n",
    "        # https://github.com/hujie-frank/SENet/blob/master/README.md\n",
    "        self.se = SELayer(in_channels, reduction=16)\n",
    "\n",
    "        # https://github.com/cfzd/FcaNet/blob/master/model/fcanet.py\n",
    "        self.fca = MultiSpectralAttentionLayer(in_channels, 7, 7, reduction=16, freq_sel_method=\"top16\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.se(x)\n",
    "        x = self.fca(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AntiAliasingBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample=True):\n",
    "        super(AntiAliasingBlock, self).__init__()\n",
    "\n",
    "        self.downsample = downsample\n",
    "\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.down_conversion = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, groups=out_channels),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        stride = 2 if self.downsample else 1\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=stride, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "        )\n",
    "\n",
    "        self.ma = MultiAttention(out_channels)\n",
    "        self.ibn = nn.InstanceNorm2d(out_channels)\n",
    "\n",
    "        self.residual_conv = None\n",
    "        if in_channels != out_channels or downsample:\n",
    "            self.residual_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "        else:\n",
    "            self.residual_conv = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.block1(x)\n",
    "        out = self.down_conversion(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.ma(out)\n",
    "        if self.residual_conv:\n",
    "            x = self.residual_conv(x)\n",
    "        out = out + x\n",
    "        out = self.ibn(out)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MaiaBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downsample=False):\n",
    "        super(MaiaBlock, self).__init__()\n",
    "\n",
    "        stride = 2 if downsample else 1\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, stride=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "        self.ma = MultiAttention(out_channels)\n",
    "        self.ibn = nn.InstanceNorm2d(out_channels)\n",
    "\n",
    "        if in_channels != out_channels or downsample:\n",
    "            self.residual_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)\n",
    "        else:\n",
    "            self.residual_conv = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.ma(out)\n",
    "\n",
    "        if self.residual_conv:\n",
    "            x = self.residual_conv(x)\n",
    "        out = out + x\n",
    "        out = self.ibn(out)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = MaiaNet(num_classes=5).to(device)\n",
    "    x = torch.randn(1, 3, 448, 448).to(device)\n",
    "    output = model(x)\n",
    "    print(output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef7d3c56-894d-4027-97e0-4886a0320c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.cuda.amp as amp  # For mixed precision training\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from tqdm import tqdm\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, test_loader, lr=0.2, num_epochs=80):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = model.to(self.device)\n",
    "\n",
    "        # Enable cudnn benchmarking for better performance\n",
    "        if torch.cuda.is_available():\n",
    "            torch.backends.cudnn.benchmark = True\n",
    "\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.num_epochs = num_epochs\n",
    "        self.lr = lr\n",
    "\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=self.lr, momentum=0.9, weight_decay=1e-5)\n",
    "        self.scheduler = ExponentialLR(self.optimizer, gamma=0.96)\n",
    "        self.criterion = nn.CrossEntropyLoss().to(self.device)  # Move loss function to GPU\n",
    "\n",
    "        # Initialize mixed precision training\n",
    "        self.scaler = torch.amp.GradScaler()\n",
    "\n",
    "        self.best_val_loss = float(\"inf\")\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        pbar = tqdm(self.train_loader, desc=f\"Epoch {epoch + 1}/{self.num_epochs}\")\n",
    "\n",
    "        for images, labels in pbar:\n",
    "            # Clear GPU cache if needed\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            images = images.to(self.device, non_blocking=True)\n",
    "            labels = labels.to(self.device, non_blocking=True)\n",
    "\n",
    "            self.optimizer.zero_grad(set_to_none=True)  # More efficient than zero_grad()\n",
    "\n",
    "            # Use mixed precision training\n",
    "            with amp.autocast():\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "\n",
    "            # Scale the loss and perform backprop\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "\n",
    "            # self.scaler.step(self.optimizer)\n",
    "            # self.scaler.update()\n",
    "            # self.optimizer.zero_grad(set_to_none=True)  # AFTER scaler update\n",
    "\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "            # Delete unnecessary tensors\n",
    "            del outputs, loss\n",
    "\n",
    "        self.scheduler.step()\n",
    "\n",
    "        return total_loss / len(self.train_loader)\n",
    "\n",
    "    @torch.no_grad()  # More efficient than with torch.no_grad()\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_preds, all_labels = [], []\n",
    "\n",
    "        for images, labels in self.val_loader:\n",
    "            images = images.to(self.device, non_blocking=True)\n",
    "            labels = labels.to(self.device, non_blocking=True)\n",
    "\n",
    "            with amp.autocast():\n",
    "                outputs = self.model(images)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Clean up GPU memory\n",
    "            del outputs, loss\n",
    "\n",
    "        avg_loss = total_loss / len(self.val_loader.dataset)\n",
    "        metrics = self.calculate_metrics(all_preds, all_labels)\n",
    "\n",
    "        return avg_loss, metrics\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_metrics(predictions, labels):\n",
    "        accuracy = accuracy_score(labels, predictions)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average=\"weighted\", zero_division=0)\n",
    "        return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "    @staticmethod\n",
    "    def print_metrics(metrics, phase):\n",
    "        print(f\"\\n{phase} Metrics:\")\n",
    "        print(\"-\" * 50)\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric.capitalize()}: {value:.4f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    def train(self):\n",
    "        try:\n",
    "            for epoch in range(self.num_epochs):\n",
    "                train_loss = self.train_epoch(epoch)\n",
    "                val_loss, val_metrics = self.validate()\n",
    "\n",
    "                print(f\"\\nEpoch {epoch + 1}: Train Loss = {train_loss:.4f} | Val Loss = {val_loss:.4f}\")\n",
    "                self.print_metrics(val_metrics, \"Validation\")\n",
    "\n",
    "                if val_loss < self.best_val_loss:\n",
    "                    self.best_val_loss = val_loss\n",
    "                    # Save model state to CPU to avoid GPU memory issues\n",
    "                    self.best_model_state = {k: v.cpu() for k, v in self.model.state_dict().items()}\n",
    "\n",
    "                # Print GPU memory usage if available\n",
    "                if torch.cuda.is_available():\n",
    "                    print(f\"GPU Memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Training interrupted: {str(e)}\")\n",
    "            # Save the current best model if training is interrupted\n",
    "            if self.best_model_state is not None:\n",
    "                torch.save(self.best_model_state, \"interrupted_model.pt\")\n",
    "\n",
    "    def test(self):\n",
    "        # Load best model state back to GPU\n",
    "        if self.best_model_state is not None:\n",
    "            self.model.load_state_dict({k: v.to(self.device) for k, v in self.best_model_state.items()})\n",
    "        test_loss, test_metrics = self.validate()\n",
    "        print(\"\\nBest Model Performance on Test Set:\")\n",
    "        self.print_metrics(test_metrics, \"Test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4477227e-0b6d-4593-bbc3-da028ad14f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, train_loader, test_loader, val_loader, 1e-4, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a9b82e-c7f5-4a31-9205-9f4e361e7dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5725/532439418.py:53: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n",
      "Epoch 1/40: 100%|██████████| 63/63 [01:25<00:00,  1.36s/it, loss=1.6206]\n",
      "/tmp/ipykernel_5725/532439418.py:87: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: Train Loss = 1.6118 | Val Loss = 1.6081\n",
      "\n",
      "Validation Metrics:\n",
      "--------------------------------------------------\n",
      "Accuracy: 0.2055\n",
      "Precision: 0.0422\n",
      "Recall: 0.2055\n",
      "F1: 0.0701\n",
      "--------------------------------------------------\n",
      "GPU Memory allocated: 2.62 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/40: 100%|██████████| 63/63 [01:16<00:00,  1.21s/it, loss=1.6312]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: Train Loss = 1.6081 | Val Loss = 1.6062\n",
      "\n",
      "Validation Metrics:\n",
      "--------------------------------------------------\n",
      "Accuracy: 0.2095\n",
      "Precision: 0.2400\n",
      "Recall: 0.2095\n",
      "F1: 0.0781\n",
      "--------------------------------------------------\n",
      "GPU Memory allocated: 2.62 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/40: 100%|██████████| 63/63 [01:16<00:00,  1.21s/it, loss=1.5919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: Train Loss = 1.6066 | Val Loss = 1.6049\n",
      "\n",
      "Validation Metrics:\n",
      "--------------------------------------------------\n",
      "Accuracy: 0.2055\n",
      "Precision: 0.0422\n",
      "Recall: 0.2055\n",
      "F1: 0.0701\n",
      "--------------------------------------------------\n",
      "GPU Memory allocated: 2.62 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/40: 100%|██████████| 63/63 [01:16<00:00,  1.21s/it, loss=1.6083]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: Train Loss = 1.6069 | Val Loss = 1.6035\n",
      "\n",
      "Validation Metrics:\n",
      "--------------------------------------------------\n",
      "Accuracy: 0.2055\n",
      "Precision: 0.0422\n",
      "Recall: 0.2055\n",
      "F1: 0.0701\n",
      "--------------------------------------------------\n",
      "GPU Memory allocated: 2.62 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/40: 100%|██████████| 63/63 [01:16<00:00,  1.21s/it, loss=1.6052]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: Train Loss = 1.6029 | Val Loss = 1.6021\n",
      "\n",
      "Validation Metrics:\n",
      "--------------------------------------------------\n",
      "Accuracy: 0.2451\n",
      "Precision: 0.2003\n",
      "Recall: 0.2451\n",
      "F1: 0.1434\n",
      "--------------------------------------------------\n",
      "GPU Memory allocated: 2.62 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/40: 100%|██████████| 63/63 [01:16<00:00,  1.22s/it, loss=1.6167]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6: Train Loss = 1.6032 | Val Loss = 1.6012\n",
      "\n",
      "Validation Metrics:\n",
      "--------------------------------------------------\n",
      "Accuracy: 0.2095\n",
      "Precision: 0.0439\n",
      "Recall: 0.2095\n",
      "F1: 0.0726\n",
      "--------------------------------------------------\n",
      "GPU Memory allocated: 2.62 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/40: 100%|██████████| 63/63 [01:16<00:00,  1.21s/it, loss=1.5872]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7: Train Loss = 1.6019 | Val Loss = 1.5979\n",
      "\n",
      "Validation Metrics:\n",
      "--------------------------------------------------\n",
      "Accuracy: 0.2925\n",
      "Precision: 0.3357\n",
      "Recall: 0.2925\n",
      "F1: 0.2296\n",
      "--------------------------------------------------\n",
      "GPU Memory allocated: 2.62 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/40: 100%|██████████| 63/63 [01:16<00:00,  1.22s/it, loss=1.5970]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8: Train Loss = 1.5988 | Val Loss = 1.5962\n",
      "\n",
      "Validation Metrics:\n",
      "--------------------------------------------------\n",
      "Accuracy: 0.2885\n",
      "Precision: 0.4138\n",
      "Recall: 0.2885\n",
      "F1: 0.1982\n",
      "--------------------------------------------------\n",
      "GPU Memory allocated: 2.62 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/40: 100%|██████████| 63/63 [01:17<00:00,  1.22s/it, loss=1.6189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9: Train Loss = 1.5970 | Val Loss = 1.5957\n",
      "\n",
      "Validation Metrics:\n",
      "--------------------------------------------------\n",
      "Accuracy: 0.3083\n",
      "Precision: 0.4274\n",
      "Recall: 0.3083\n",
      "F1: 0.2093\n",
      "--------------------------------------------------\n",
      "GPU Memory allocated: 2.62 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/40: 100%|██████████| 63/63 [01:16<00:00,  1.22s/it, loss=1.6052]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10: Train Loss = 1.5958 | Val Loss = 1.5930\n",
      "\n",
      "Validation Metrics:\n",
      "--------------------------------------------------\n",
      "Accuracy: 0.3123\n",
      "Precision: 0.2284\n",
      "Recall: 0.3123\n",
      "F1: 0.2381\n",
      "--------------------------------------------------\n",
      "GPU Memory allocated: 2.62 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/40: 100%|██████████| 63/63 [01:16<00:00,  1.21s/it, loss=1.5938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11: Train Loss = 1.5935 | Val Loss = 1.5924\n",
      "\n",
      "Validation Metrics:\n",
      "--------------------------------------------------\n",
      "Accuracy: 0.3597\n",
      "Precision: 0.3786\n",
      "Recall: 0.3597\n",
      "F1: 0.3160\n",
      "--------------------------------------------------\n",
      "GPU Memory allocated: 2.62 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/40:  25%|██▌       | 16/63 [00:19<00:55,  1.19s/it, loss=1.5981]"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2b5f66-e34d-4981-8598-7463d7fd6fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer.train_epoch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9942f2b7-ed09-40fa-a64f-876973270ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
